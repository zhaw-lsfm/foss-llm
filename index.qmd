---
author: FG Geoinformatik
format:
  revealjs:
    theme: dark
    css:
    - styles/base.css
    - styles/custom.css
    slide-number: true
    transition: slide
    background-transition: fade
    pagetitle: OSS LLMs
    code-line-numbers: false
    footer: '<a href="https://webgis.lsfm.zhaw.ch/svu-asep-webinar">webgis.lsfm.zhaw.ch/svu-asep-webinar</a>'
    chalkboard: true
revealjs-plugins:
  - attribution
lang: de
---

# Mehr als nur ChatGPT {background-image="images/5790836212_87a89fe3b9_o.jpg" class="blurred-10px strokeme"}

[Was Open Source und On Premise KI uns bieten können]{.strokeme}

[Slides: [webgis.lsfm.zhaw.ch/svu-asep-webinar](https://webgis.lsfm.zhaw.ch/svu-asep-webinar)]{.strokeme}


::: {.attribution}
Photo courtesy of Leonard Bentley, CC BY-SA 2.0
:::

## {background-image="images/5790836212_87a89fe3b9_o.jpg" .shadow}

::: {.attribution}
Photo courtesy of Leonard Bentley, CC BY-SA 2.0
:::

 <!-- [@ingtotheforest](https://unsplash.com/@ingtotheforest) -->


:::{.notes}
- Speakers corner in Hyde Park, London
- They shout at each other, they shout at their listeners, and sometimes they just shout at the squirrels.


:::

##

::::{.profile-layout}
:::{}
![](images/rata.jpeg){#rata}
:::

:::{#portrait}
**Nils Ratnaweera**  
Spatial Data Scientist  

Wissenschaftlicher Mitarbeiter [FG Geoinformatik (ZHAW)](https://www.zhaw.ch/de/lsfm/institute-zentren/iunr/geooekologie/geoinformatik)  
Selbständig ([Ratnaweera.xyz](https://www.ratnaweera.xyz))

:::

::::
:::{.notes}

- Weder AI Experte noch KI-Apologet (Person, die mit ihrer ganzen Überzeugung hinter einer Auffassung oder Lehre steht und diese mit Nachdruck nach außen vertritt)
- Nicht mal ein KI *early adopter*
- Warum halte ich diesen Talk? 
:::

## Drei Thesen

:::{.incremental}
1. KI wird unsere Arbeitsweise grundlegend verändern
2. Wir müssen diese Veränderung aktiv mitgestalten
3. Open Source ist der Schlüssel zur Mitgestaltung von KI
:::

:::{.notes}

1. These
  - Die meisten Aufgaben, die auf einem Computer gelöst werden, können mit Code / Scripting gelöst werden
  - Alle Aufgaben, die mit code gelöst werden können, können mit KI ein vielfaches schneller gelöst werden
  


:::



## 


![Umfrage zur Nutzung von KI bei der Anmeldung an das heutige Webinar](images/umfrage-sketch.svg){#fig-umfrage}


## {background-image="images/VR_and_Greshan3.png"}


:::{.notes}
- Was verbindet mich mit dem Thema OpenSource?
- Foto 1998 wo mein Vater ein Workshop zu OpenSource Software leitet
- Wir lebten damals in Sri Lanka und mein Vater dozierte an der University of Peradeniya
- In Sri Lanka ist der Bedarf an OSS noch eine ganz andere als hier, wo Lizenzkosten keine Rolle zu spielen scheinen

:::


## {background-image="images/table-setting-6859276\_1280.jpg"}

::: {.notes}

**Metapher aus der Küche**

- Stellen Sie sich vor, Sie wurden zu einem fantastischen Abendessen bei einem entfernten Verwandten eingeladen.
- Sie fragen, wie das Abendessen zubereitet wurde, und der Gastgeber zeigt Ihnen stolz das Rezept
- (nächste Folie)

:::

## {background-image="images/thermomix.png" background-size="contain" background-color="white"}

## {background-color="#5F9EA0"}

:::: {.recipe}
REZEPT

1. 10 Einheiten Compound-XJ7 in den Mixtopf geben, 5 Sek./Stufe 5 zerkleinern.
2. 4 Einheiten Carrier-Oil-Premium™ hinzufügen, 3 Min./120°C/Stufe 1 erhitzen.
3. Rezept-Chip einlegen (separat erhältlich) und Guided-Cooking starten.
4. Bei grüner Kontrollleuchte 1 Portion Flavor-Packet-B hinzufügen.



⚠ WARNUNG:

- Nur zertifizierte Zutaten verwenden (Garantie!)
- Rezept erfordert aktives Cloud-Abo (49€/Jahr)
- *ThermoMax-2025* wird nächstes Jahr eingestellt; Upgrade erforderlich

::::

::: {.notes}

- Hinweis: proprietäre Zutaten, vorgeschriebene Geräte, Abo-Modell, geplante Obsoleszenz. Das sollte sich lächerlich anhören.

:::


## {background-color="#5F9EA0"}

::: {.recipe}
OFFENES REZEPT

1. Zwiebeln klein schneiden, in Öl glasig dünsten (ca. 3-5 Min.).

2. Knoblauch dazu, kurz anbraten bis es duftet.

3. Tomaten hinzufügen, 20 Minuten köcheln lassen.

HINWEISE:

- Keine Zwiebeln? Schalotten oder Lauch funktionieren auch.
- Jedes Öl oder Butter geht – was Sie zuhause haben.
- Jede Pfanne funktioniert. Mit Deckel geht's schneller.

:::

::: {.notes}


- Verständnis vs. blindes Befolgen. Beachten Sie die Flexibilität 
- Wenn man ihn darauf anspricht, hat der Gastgeber folgende Argumente:
  - Mit dieser Methode weiß ich, dass das Essen großartig wird. Ich habe nichts dagegen, dafür ein wenig Freiheit zu opfern.
  - Wenn ich selbst kochen wollte, müsste ich so viele Details lernen. Dafür habe ich keine Zeit.
  - Alle, die ich kenne, kochen so. *Das wurde uns in der Schule so beigebracht.*
- Wir würden ein solches System im Bereich des Kochens *NIEMALS* tolerieren

:::

## Kochen vs. Computer



![Täglich investierte Zeit: Kochen vs. Computer nach Ratnaweera et al (2025), n = 2](images/kochen-vs-computer-sketch.svg){#fig-kochen-vs-computer}

::: {.notes}

- Wir verbringen *viel* mehr Zeit am Computer als mit Kochen.
- Warum lassen wir das im digitalen Bereich zu?

:::


## Vendor Lock-In


![](images/Push-and-pull.jpg)



::: {.notes}

- Anbieter wollen nicht, dass Sie plattformunabhängig sind.
- Sie machen ihre Produkte absichtlich mit denen anderer Anbieter inkompatibel.
- Sobald ein bestimmte Investition gemacht ist, ist es schwierig oder sogar unmöglich, den Anbieter zu wechseln.
- (zurück zur Koch-analogie)
:::

## {background-color="#5F9EA0"}

:::: {.recipe}
REZEPT

1. 10 Einheiten Compound-XJ7 in den Mixtopf geben, 5 Sek./Stufe 5 zerkleinern.
2. 4 Einheiten Carrier-Oil-Premium™ hinzufügen, 3 Min./120°C/Stufe 1 erhitzen.
3. Rezept-Chip einlegen (separat erhältlich) und Guided-Cooking starten.
4. Bei grüner Kontrollleuchte 1 Portion Flavor-Packet-B hinzufügen.



⚠ WARNUNG:

- Nur zertifizierte Zutaten verwenden (Garantie!)
- Rezept erfordert aktives Cloud-Abo (49€/Jahr)
- *ThermoMax-2025* wird nächstes Jahr eingestellt; Upgrade erforderlich

::::

::: {.notes}
- Sobald das Gerät, das Cloud-Abo und die zertifizierten Zutaten gekauft wurden, wird es schwierig sein, Rezepte anderer Anbieter zu verwenden

- **Warum ist das ein Problem?**
:::

## {background-image="images/sanctions.png" background-size="contain" background-color="white"}

<!-- https://www.heise.de/news/Strafgerichtshof-Microsofts-E-Mail-Sperre-als-Weckruf-fuer-digitale-Souveraenitaet-10387368.html -->

::: {.notes}

- Microsoft hat nach Trump-Sanktionen das Mail-Konto des Chefanklägers des Internationalen Gerichtshofs blockiert.
- Trump sanktionierte das Den Haager Gericht im Februar, nachdem ein Gremium von IStGH-Richtern im November 25 Haftbefehle gegen den israelischen Premierminister Benjamin Netanjahu mit Blick auf Kriegsverbrechen im Gaza-Streifen erlassen hatte.
:::

## {background-image="images/end-of-10.png" background-size="contain" background-color="white"}

<!-- https://www.computerbild.de/artikel/cb-Tipps-Software-Windows-10-wird-eingestellt-Updates-36482825.html -->

::: {.notes}

- Manchmal werden Softwareversionen nicht mehr unterstützt, wie das Beispiel des End-of-Life für Windows 10 zeigt.
- Das Problem: Zahlreiche PCs bringen die geforderten Komponenten nicht mit. Ein Upgrade auf Windows 11 auf offiziellem Wege ist in solchen Fällen ausgeschlossen.
:::


## {background-image="images/end-of-10-2.png" background-color="#2a67b2" background-size="contain"}

::: {.notes}
- Dazu noch eine Side-Note: Tausende von Freiwillige bieten workshops an um den Umstieg von Windows 10 auf Linux zu vereinfachen. 
- Mein Vater hilft natürlich mit :-)
- (Überleitung: Einwand)
:::

## {.center visibility="hidden"}

:::{.blockquote}
Aber nicht jeder kann ein Computerexperte sein
:::

::: {.notes}

- Die "10'000-Stunden-Regel", die Malcolm Gladwell in seinem Buch "Outliers" populär gemacht hat, besagt, dass man etwa 10.000 Stunden Übung benötigt, um eine Fertigkeit zu meistern.
- Bei 6 Stunden pro Tag sind Sie nach etwa 7 Jahren ein Computerexperte
- Insbesondere müssen wir nicht alle 3 Ebenen verstehen (nächste Folie)

:::

## Drei Ebenen{.center visibility="hidden"}

:::: {.fragment .fade-in-then-semi-out}
::: {.layer-box .layer-box-blue}
**Interface**
:::
::::

:::: {.fragment .fade-in-then-semi-out}
::: {.layer-box .layer-box-orange}
**Implementierung**
:::
::::

:::: {.fragment}
::: {.layer-box  .layer-box-purple}
**Technologie** 
:::
::::



:::{.notes}
- Interface - Für alle
- Implementierung - Für Anwender
- Technologie - Für Experten
:::

## Am Scheideweg {data-background-image="images/fork-in-the-road-624151138_f1ff60b2db_o-1950536461.jpg" .center .shadow .scheideweg}

::: {.notes}

- Gehen wir *den einfachen* oder *den richtigen Weg*?
- Werden wir *Meister einer Technologie* oder *Nutzer eines Produkts*?

:::




# Openweights AI {background-image="images/console-4567918_1280.jpg" .shadow .blurred-10px}

## {background-image="images/console-4567918_1280.jpg"}

:::{.notes}
Gewichte sind die gelernten Zahlen innerhalb eines neuronalen Netzwerks.                      
- Stellen Sie sich das so vor: Ein neuronales Netzwerk besteht im Grunde aus Millionen/Milliarden winziger „Regler“, die während des Trainings angepasst werden. Jeder Regler steuert, wie stark eine Information eine andere beeinflusst.                                     
- Wenn Sie ein Modell anhand von Text trainieren, stellen Sie im Wesentlichen alle diese Knöpfe ein     , bis das Modell gute Ergebnisse liefert. Die endgültigen Einstellungen all dieser Knöpfe = die Gewichte.                                                                 
- Wenn also jemand von einem „7B-Modell” spricht, bedeutet das 7 Milliarden Gewichte (7 Milliarden Zahlen, die während des Trainings gelernt wurden).                                   
   

:::


## "Offenheit" ist ein Spektrum

offener ↑

::: {.incremental .two-col-list}

- [Vollständig offen]{}  [(Gewichte ✔ Code ✔ Daten ✔)]{}
- [Offener Code]{} [(Gewichte ✔ Code ✔ Daten ✕)]{}
- [Offene Gewichte]{}  [(Gewichte ✔ Code ✕ Daten ✕)]{}
- [Proprietär]{} [(Gewichte ✕ Code ✕ Daten ✕)]{}
:::

geschlossener ↓

::: {.notes}

                                                                             

- **Proprietär (ChatGPT, Claude, Gemini):**
  - Komplette Black Box – Sie erhalten nur API-Zugriff
  - Keine Einblicke in die Funktionsweise, die verwendeten Trainingsdaten oder die Entscheidungsfindung
  - Sie sind vollständig vom Anbieter abhängig
- **Offene Gewichte (Llama 3.3, DeepSeek, Mistral, Qwen):**
  - Die Gewichte des trainierten Modells werden veröffentlicht – Sie können sie herunterladen und lokal ausführen
  - ABER: Trainingscode und Trainingsdaten sind in der Regel NICHT enthalten
  - Sie können es VERWENDEN, aber nicht vollständig überprüfen oder reproduzieren, wie es erstellt wurde
  - Hier befinden sich heute die meisten "offenen" Modelle
  - Für praktische Lehrzwecke: Das ist ausreichend! Sie erhalten lokale Kontrolle.
- **Offene Gewichte + Code (einige Forschungsveröffentlichungen):**
  - Gewichte + die für das Training verwendete Methodik/der verwendete Code
  - Sie können nachvollziehen und überprüfen, WIE das Training durchgeführt wurde
  - ABER: Trainingsdaten sind in der Regel nicht enthalten (Lizenzprobleme, Wettbewerbsvorteil)
  - Beispiel: Veröffentlichungen einiger akademischer Labore
  - Näher an der echten "Open Source"-Philosophie – transparent und überprüfbar
- **Vollständig Open Source (OLMo, BLOOM – selten!):**
  - Gewichte + Trainingscode + Trainingsdaten – alles
  - Vollständig reproduzierbar und überprüfbar
  - Dies ist der Goldstandard, aber extrem selten (Trainingsdaten sind teuer, komplexe Lizenzierung)
  - Am ähnlichsten zu traditioneller Open-Source-Software

:::




## 

![Geschätzte Anzahl "Open Weights" Modelle auf huggingface.co](images/hugging-face-over-time-sketch.svg){#fig-hf}


## {background-image="images/huggingface.png" background-size="contain" background-color="white"}

:::{#huggingface}                                                             
[](https://huggingface.co)                                                    
:::  

## OpenWeights Cloud AI

- [mistral.ai](https://mistral.ai)
- [ollama.ai](https://ollama.ai)
- [qwen.ai](https://qwen.ai)
- [deepseek.com](https://deepseek.com)
- [Gemma](https://deepmind.google/models/gemma/)


:::{.notes}

- Mistral ist EU basiert (FR)
- qwen ist aus China (Alibaba Cloud)
- ollama cloud ist CLI basiert, und kann auch lokale AI models verwenden
- deepseek ist aus China. Eine der ersten OS AI Models, die aufgrund von Einschränkungen in der Grafikkarte mit weniger Leistung klarkommen mussten
- Gemma ist von Google


:::



# Praktische Demo 1 {background-image="images/magic-note-8836935\_1280.jpg" .shadow .blurred-5px}

[OpenSource Lokal]{.shadow}

## {background-image="images/magic-note-8836935\_1280.jpg"}


:::{.notes}

- Opensource Lokale KI
- OS Cloud KI sind zwar super, aber mit vorsicht zu geniessen. Sensible Daten wandern trotzdem ab und können für Trainingszwecken verwendet werden
- Lokale LLMs sind hier die Lösung

- Wir arbeiten erstmals im Terminal.
- Steiler einstieg, es gibt auch GUIS
- aber der Terminal öffnet uns Tür und Tor für FOSS

:::


## Ollama {background-image="images/welcome.avif" background-color="white"}



## Ollama{background-color="white"}


![](images/ollama.png)



::: {.notes}

**Windows-Installation:**

1. Von [ollama.com/download](https://ollama.com/download) **OllamaSetup.exe** herunterladen
2. OllamaSetup.exe mit default Einstellungen installieren 
:::


## Terminal Öffnen

- Windows: *Command Prompt*
- Mac: *Terminal*
- Linux: *Terminal*


::: {.notes}
- Wenn der Befehl nicht gefunden wird, starten Sie den Computer neu
:::


## Im Terminal


:::{.terminal}
```{.bash}
ollama --version          
```
```{.txt}
ollama version is 0.14.2
```
:::

:::{.notes}

- Der Terminal bietet den Zugang zu OpenSource Software
- Ohne Terminal ist nur ein Bruchteil der FOSS nutzbar
- Der Terminal wirkt anfänglich befremdlich und umständlich, aber mit KI wird die Benutzung vereinfacht
- `ollama` ruft das Programm "ollama" auf
- `--version` ist ein Argument zum Program `ollama`
:::


## Im Terminal


:::{.terminal}
```{.bash}
ollama --help          
```
```{.txt}
Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  signin      Sign in to ollama.com
  signout     Sign out from ollama.com
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.

```
:::

:::{.notes}
- `ollama` ruft das Programm "ollama" auf
- `--help` ist ein Argument zum Program `ollama` und ruft die Hilfestellung zu dieser Software auf
- fast alle terminal programme haben die argumente `--version` und `--help`
:::

## RAM Verfügbarkeit


:::{.terminal}
```{.bash}
systeminfo 
```
```{.txt}
[...]
Gesamter physischer Speicher:                  32’717 MB
[...]
```
:::

::: {.notes}
- Die meisten modernen Laptops haben mindestens 8 GB RAM
- Für den Workshop reichen kleine Modelle (1.5B - 3B) völlig aus
- Mehr RAM = größere Modelle = bessere Qualität (aber langsamer)

**Interpretation:**

- **8 GB oder weniger**: Kleine Modelle (1.5B - 3B) oder Cloud
- **16 GB**: Mittlere Modelle (7B) möglich ✅
- **32 GB+**: Große Modelle (13B+) möglich
:::

## GPU Verfügbarkeit

:::{.terminal}
```{.bash}
wmic path win32_VideoController get name
```

:::


::: {.notes}
- NVIDIA GPUs beschleunigen die Inferenz erheblich
- AMD/Intel GPUs werden von Ollama noch nicht gut unterstützt
- Ohne GPU dauern Antworten etwas länger, funktionieren aber trotzdem
- Für den Unterricht ist CPU-basierte Inferenz völlig ausreichend
:::

## Modellwahl

- Falls Sie mindestens 8 GB RAM haben
  - Suchen Sie auf [ollama.ai](https://ollama.ai) ein kleines Model (1 - 2b Parameter) heraus
  - Z.B.
    - `llama3.2`
    - `deepseek-r1`

---
    
- Falls Sie weniger als 8 GB RAM haben
  1. Erstellen Sie ein kostenloses Konto auf [ollama.com](https://ollama.com)
  2. Melden Sie sich in der Kommandozeile an

:::{.terminal}
```{.bash}
ollama signin
```
:::

:::{.notes}

- Bei gewissen Modellen (z.B: deepseek-r1) gibt es verschiedene Grössen (1 - 700b). 
- Das sind die Anzahl Parameter (in Milliarden). 
- Wir starten mit einem kleinen Modell, indem wir mit einem Klick den Namen kopieren.

:::


##

Mit folgendem Kommandozeilenbefehl kann man das Modell (egal ob lokal oder cloud) initialisieren

:::{.terminal}
```{.bash}
# initialisiert ein kleines, lokales Modell
ollama pull deepseek-r1:1.5b     


# initialisiert ein grosses cloud Modell
ollama pull ministral   
```
:::


::: {.notes}
- `deepseek-r1:1.5b`: ~1 GB Download, sehr schnell, gut für logisches Denken
- `llama3.2:3b`: ~2 GB Download, vielseitig, populär
- `qwen2.5:3b`: ~2 GB Download, besser bei nicht-englischen Sprachen
- Der Download kann je nach Internetverbindung 1-5 Minuten dauern


```powershell
ollama pull gemini-flash      # Schnell und kosteneffizient
ollama pull deepseek-v3       # Sehr leistungsfähig
ollama pull ministral         # Vielseitig
```
:::





## Chat initialisieren


:::{.terminal}
```{.bash}
# Lokales Modell:
ollama run deepseek-r1:1.5b


# ODER Cloud-Modell:
ollama run gemini-flash
```
:::


## Erster Prompt


:::{.terminal}
```{.txt}
>>> Explain a group of environmental professionals the benifits of 
OpenSource Large Language Models
```
:::



::: {.notes}
**Wichtige Punkte:**
- `ollama run <modell>` startet eine Chat-Sitzung (cloud oder lokal)
- Die erste Abfrage kann etwas länger dauern Folgeabfragen sind schneller
- Beenden Sie den Chat mit `/bye` oder Strg+C


**Was passiert hinter den Kulissen (bei lokalen Modellen):**

1. Die Modelldatei wird in den RAM geladen
2. Ihr Text wird in Zahlen (Tokens) umgewandelt
3. Das Modell (die Gewichte) berechnet das nächste wahrscheinlichste Token
4. Die Ergebnisse werden wieder in Text umgewandelt
5. Alles geschieht **lokal auf Ihrem Computer** - komplett offline!


Jetzt haben wir einen etwas langsamen und nicht sehr intelligenten Chatbox.. was hat uns das gebracht?
:::





## Die drei Verbesserungen{visibility="hidden"}

::: {.columns}
::: {.column width="33%"}
**RAG**

Retrieval Augmented Generation

Geben Sie LLM Zugriff auf IHRE Dokumente

*Beispiel: IUNR-Kursmaterialien, Forschungsarbeiten*
:::

::: {.column width="33%"}
**MCPs**

Modellkontextprotokoll

Verbinden Sie LLM mit Ihren Tools

*Beispiel: Felddaten, GIS, E-Mail, Kalender*
:::

::: {.column width="33%"}
**Personas**

Systemaufforderungen

Gestalten Sie das Verhalten von LLM

*Beispiel: "Lehrassistent für Umweltwissenschaften"*
:::
:::

::: {.notes}
Diese drei Verbesserungen machen lokale LLMs leistungsstark. Gehen Sie nicht zu sehr ins Technische – erklären Sie einfach, was jede einzelne Funktion bewirkt, und geben Sie konkrete Beispiele, die für den Unterricht relevant sind. RAG ist für ihren Anwendungsfall besonders wichtig.
:::

## Warum lokale LLMs für den Unterricht sinnvoll sind{visibility="hidden"}

- Studierendendaten sind**sensibel**(DSGVO, Datenschutz)
- **Sich wiederholende Aufgaben**(Benotung, Feedback)
- Maßgeschneiderte**Kursmaterialien**(RAG)
- Einmalige Einrichtung,**dauerhafter Nutzen**
- **Keine Kosten pro Abfrage**

::: {.notes}
Verbinden Sie die Punkte: Studentendaten + DSGVO = muss lokal sein. Sich wiederholende Aufgaben + keine Kosten pro Abfrage = wirtschaftlich rentabel. Deshalb ist lokal speziell für den Unterricht sinnvoll.
:::

## Übergang{visibility="hidden"}

::: {.r-fit-text}
Okay, was können Siedamit**konkret machen**?
:::

::: {.notes}
Übergang zu praktischen Anwendungen. Wechsel von "Warum" zu "Was". Zeit, konkreten Wert zu zeigen.
:::




# Praktische Demo 2 {background-image="images/magic-note-8836935\_1280.jpg" .shadow .blurred-5px}

[LLMs via einem GUI]{.shadow}

:::{.notes}

- Anything LLM

:::

## {background-image="images/magic-note-8836935\_1280.jpg"}


## AnythingLLM

- Anything LLM ist eine ideale Ergänzung zu Ollama
- Die Software bietet ein grafisches Interface und verwendet die Modelle, die von Ollama zu Verfügung gestellt werden
- AnythingLLM ist frei und OpenSource, und für alle 3 Plattformen verfügbar



## Demo

:::{.notes}

- Zeige, wie Ollama als Model provider verwendet werden kann
- Zeige, wie eigene Daten als RAG verwendet werden können
- Zeige, wie die eigenen Daten abgefragt werden können

:::




