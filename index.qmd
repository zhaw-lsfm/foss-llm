---
author: FG Geoinformatik
format:
  revealjs:
    theme: dark
    css:
    - styles/base.css
    - styles/custom.css
    slide-number: true
    transition: slide
    background-transition: fade
    pagetitle: OSS LLMs
    code-line-numbers: false
    footer: '<a href="https://webgis.lsfm.zhaw.ch/svu-asep-webinar">webgis.lsfm.zhaw.ch/svu-asep-webinar</a>'
    chalkboard: true
revealjs-plugins:
  - attribution
lang: de
---

# Mehr als nur ChatGPT {background-image="images/5790836212_87a89fe3b9_o.jpg" class="blurred-10px strokeme hydepark center" footer=false}

[Was Open Source und On Premise KI uns bieten können]{.strokeme}

![Slides: [webgis.lsfm.zhaw.ch/svu-asep-webinar](https://webgis.lsfm.zhaw.ch/svu-asep-webinar%5D)](images/adobe-express-qr-code.svg){width=100}


::: {.attribution}
Photo courtesy of Leonard Bentley, CC BY-SA 2.0
:::

## {background-image="images/5790836212_87a89fe3b9_o.jpg" .shadow}

::: {.attribution}
Photo courtesy of Leonard Bentley, CC BY-SA 2.0
:::

 <!-- [@ingtotheforest](https://unsplash.com/@ingtotheforest) -->


:::{.notes}
- Speakers corner in Hyde Park, London
- They shout at each other, they shout at their listeners, and sometimes they just shout at the squirrels.


:::

##

::::{.profile-layout}
:::{}
![](images/rata.jpeg){#rata}
:::

:::{#portrait}
**Nils Ratnaweera**  
Spatial Data Scientist  

Wissenschaftlicher Mitarbeiter [FG Geoinformatik (ZHAW)](https://www.zhaw.ch/de/lsfm/institute-zentren/iunr/geooekologie/geoinformatik)  
Selbständig ([Ratnaweera.xyz](https://www.ratnaweera.xyz))

:::

::::
:::{.notes}

- Weder AI Experte noch KI-Apologet (Person, die mit ihrer ganzen Überzeugung hinter einer Auffassung oder Lehre steht und diese mit Nachdruck nach außen vertritt)
- Nicht mal ein KI *early adopter*
- Warum halte ich diesen Talk? 
:::


## 


![Umfrage zur Nutzung von KI bei der Anmeldung an das heutige Webinar](images/umfrage-sketch.svg){#fig-umfrage}


## Drei Thesen

:::{.incremental}
1. KI wird unsere Arbeitsweise grundlegend verändern
2. Wir müssen diese Veränderung aktiv mitgestalten
3. Open Source ist der Schlüssel zur Mitgestaltung von KI
:::

:::{.notes}

1. These
  - Die meisten Aufgaben, die auf einem Computer gelöst werden, können mit Code / Scripting gelöst werden
  - Alle Aufgaben, die mit code gelöst werden können, können mit KI ein vielfaches schneller gelöst werden
  


:::






## {background-image="images/VR_and_Greshan3.png"}


:::{.notes}
- Was verbindet mich mit dem Thema OpenSource?
- Foto 1998 wo mein Vater ein Workshop zu OpenSource Software leitet
- Wir lebten damals in Sri Lanka und mein Vater dozierte an der University of Peradeniya
- In Sri Lanka ist der Bedarf an OSS noch eine ganz andere als hier, wo Lizenzkosten keine Rolle zu spielen scheinen

:::


## {background-image="images/table-setting-6859276\_1280.jpg"}

::: {.notes}

**Metapher aus der Küche**

- Stellen Sie sich vor, Sie wurden zu einem fantastischen Abendessen bei einem entfernten Verwandten eingeladen.
- Sie fragen, wie das Abendessen zubereitet wurde, und der Gastgeber zeigt Ihnen stolz das Rezept
- (nächste Folie)

:::

## {background-image="images/thermomix.png" background-size="contain" background-color="white"}

## {background-color="#5F9EA0"}

:::: {.recipe}
REZEPT

1. 10 Einheiten Compound-XJ7 in den Mixtopf geben, 5 Sek./Stufe 5 zerkleinern.
2. 4 Einheiten Carrier-Oil-Premium™ hinzufügen, 3 Min./120°C/Stufe 1 erhitzen.
3. Rezept-Chip einlegen (separat erhältlich) und Guided-Cooking starten.
4. Bei grüner Kontrollleuchte 1 Portion Flavor-Packet-B hinzufügen.



⚠ WARNUNG:

- Nur zertifizierte Zutaten verwenden (Garantie!)
- Rezept erfordert aktives Cloud-Abo (49€/Jahr)
- *ThermoMax-2025* wird nächstes Jahr eingestellt; Upgrade erforderlich

::::

::: {.notes}

- Hinweis: proprietäre Zutaten, vorgeschriebene Geräte, Abo-Modell, geplante Obsoleszenz. Das sollte sich lächerlich anhören.

:::


## {background-color="#5F9EA0"}

::: {.recipe}
OFFENES REZEPT

1. Zwiebeln klein schneiden, in Öl glasig dünsten (ca. 3-5 Min.).

2. Knoblauch dazu, kurz anbraten bis es duftet.

3. Tomaten hinzufügen, 20 Minuten köcheln lassen.

HINWEISE:

- Keine Zwiebeln? Schalotten oder Lauch funktionieren auch.
- Jedes Öl oder Butter geht – was Sie zuhause haben.
- Jede Pfanne funktioniert. Mit Deckel geht's schneller.

:::

::: {.notes}


- Verständnis vs. blindes Befolgen. Beachten Sie die Flexibilität 
- Wenn man ihn darauf anspricht, hat der Gastgeber folgende Argumente:
  - Mit dieser Methode weiß ich, dass das Essen großartig wird. Ich habe nichts dagegen, dafür ein wenig Freiheit zu opfern.
  - Wenn ich selbst kochen wollte, müsste ich so viele Details lernen. Dafür habe ich keine Zeit.
  - Alle, die ich kenne, kochen so. *Das wurde uns in der Schule so beigebracht.*
- Wir würden ein solches System im Bereich des Kochens *NIEMALS* tolerieren

:::

## Kochen vs. Computer



![Täglich investierte Zeit: Kochen vs. Computer nach Ratnaweera et al (2025), n = 2](images/kochen-vs-computer-sketch.svg){#fig-kochen-vs-computer}

::: {.notes}

- Wir verbringen *viel* mehr Zeit am Computer als mit Kochen.
- Warum lassen wir das im digitalen Bereich zu?

:::


## Vendor Lock-In


![](images/Push-and-pull.jpg)



::: {.notes}

- Anbieter wollen nicht, dass Sie plattformunabhängig sind.
- Sie machen ihre Produkte absichtlich mit denen anderer Anbieter inkompatibel.
- Sobald ein bestimmte Investition gemacht ist, ist es schwierig oder sogar unmöglich, den Anbieter zu wechseln.
- (zurück zur Koch-analogie)
:::

## {background-color="#5F9EA0"}

:::: {.recipe}
REZEPT

1. 10 Einheiten Compound-XJ7 in den Mixtopf geben, 5 Sek./Stufe 5 zerkleinern.
2. 4 Einheiten Carrier-Oil-Premium™ hinzufügen, 3 Min./120°C/Stufe 1 erhitzen.
3. Rezept-Chip einlegen (separat erhältlich) und Guided-Cooking starten.
4. Bei grüner Kontrollleuchte 1 Portion Flavor-Packet-B hinzufügen.



⚠ WARNUNG:

- Nur zertifizierte Zutaten verwenden (Garantie!)
- Rezept erfordert aktives Cloud-Abo (49€/Jahr)
- *ThermoMax-2025* wird nächstes Jahr eingestellt; Upgrade erforderlich

::::

::: {.notes}
- Sobald das Gerät, das Cloud-Abo und die zertifizierten Zutaten gekauft wurden, wird es schwierig sein, Rezepte anderer Anbieter zu verwenden

- **Warum ist das ein Problem?**
:::

## {background-image="images/sanctions.png" background-size="contain" background-color="white"}

<!-- https://www.heise.de/news/Strafgerichtshof-Microsofts-E-Mail-Sperre-als-Weckruf-fuer-digitale-Souveraenitaet-10387368.html -->

::: {.notes}

- Microsoft hat nach Trump-Sanktionen das Mail-Konto des Chefanklägers des Internationalen Gerichtshofs blockiert.
- Trump sanktionierte das Den Haager Gericht im Februar, nachdem ein Gremium von IStGH-Richtern im November 25 Haftbefehle gegen den israelischen Premierminister Benjamin Netanjahu mit Blick auf Kriegsverbrechen im Gaza-Streifen erlassen hatte.
:::

## {background-image="images/end-of-10.png" background-size="contain" background-color="white"}

<!-- https://www.computerbild.de/artikel/cb-Tipps-Software-Windows-10-wird-eingestellt-Updates-36482825.html -->

::: {.notes}

- Manchmal werden Softwareversionen nicht mehr unterstützt, wie das Beispiel des End-of-Life für Windows 10 zeigt.
- Das Problem: Zahlreiche PCs bringen die geforderten Komponenten nicht mit. Ein Upgrade auf Windows 11 auf offiziellem Wege ist in solchen Fällen ausgeschlossen.
:::


## {background-image="images/end-of-10-2.png" background-color="#2a67b2" background-size="contain"}

::: {.notes}
- Dazu noch eine Side-Note: Tausende von Freiwillige bieten workshops an um den Umstieg von Windows 10 auf Linux zu vereinfachen. 
- Mein Vater hilft natürlich mit :-)
- (Überleitung: Einwand)
:::

## {.center visibility="hidden"}

:::{.blockquote}
Aber nicht jeder kann ein Computerexperte sein
:::

::: {.notes}

- Die "10'000-Stunden-Regel", die Malcolm Gladwell in seinem Buch "Outliers" populär gemacht hat, besagt, dass man etwa 10.000 Stunden Übung benötigt, um eine Fertigkeit zu meistern.
- Bei 6 Stunden pro Tag sind Sie nach etwa 7 Jahren ein Computerexperte
- Insbesondere müssen wir nicht alle 3 Ebenen verstehen (nächste Folie)

:::

## Drei Ebenen{.center visibility="hidden"}

:::: {.fragment .fade-in-then-semi-out}
::: {.layer-box .layer-box-blue}
**Interface**
:::
::::

:::: {.fragment .fade-in-then-semi-out}
::: {.layer-box .layer-box-orange}
**Implementierung**
:::
::::

:::: {.fragment}
::: {.layer-box  .layer-box-purple}
**Technologie** 
:::
::::



:::{.notes}
- Interface - Für alle
- Implementierung - Für Anwender
- Technologie - Für Experten
:::

## Am Scheideweg {data-background-image="images/fork-in-the-road-624151138_f1ff60b2db_o-1950536461.jpg" .center .shadow .scheideweg}

::: {.notes}

- Gehen wir *den einfachen* oder *den richtigen Weg*?
- Werden wir *Meister einer Technologie* oder *Nutzer eines Produkts*?

:::




# Openweights AI {background-image="images/console-4567918_1280.jpg" .shadow .blurred-10px}

## {background-image="images/console-4567918_1280.jpg"}

:::{.notes}
Gewichte sind die gelernten Zahlen innerhalb eines neuronalen Netzwerks.     
- Stellen Sie sich das so vor: Ein neuronales Netzwerk besteht im Grunde aus Millionen/Milliarden winziger „Regler“, die während des Trainings angepasst werden. Jeder Regler steuert, wie stark eine Information eine andere beeinflusst.   
- Wenn Sie ein Modell anhand von Text trainieren, stellen Sie im Wesentlichen alle diese Knöpfe ein     , bis das Modell gute Ergebnisse liefert. Die endgültigen Einstellungen all dieser Knöpfe = die Gewichte.              
- Wenn also jemand von einem „7B-Modell” spricht, bedeutet das 7 Milliarden Gewichte (7 Milliarden Zahlen, die während des Trainings gelernt wurden). 
   

:::


## "Offenheit" ist ein Spektrum {.nostrech}

offener ↑

::: {.incremental .two-col-list}

- [Vollständig offen]{}  [(Gewichte ✔ Code ✔ Daten ✔)]{}
- [Offener Code]{} [(Gewichte ✔ Code ✔ Daten ✕)]{}
- [Offene Gewichte]{}  [(Gewichte ✔ Code ✕ Daten ✕)]{}
- [Proprietär]{} [(Gewichte ✕ Code ✕ Daten ✕)]{}
:::

geschlossener ↓




::: {.notes}

         

- **Proprietär (ChatGPT, Claude, Gemini):**
- **Offene Gewichte (Llama 3.3, DeepSeek, Mistral, Qwen):**
- **Offene Gewichte + Code (einige Forschungsveröffentlichungen):**
- **Vollständig Open Source (OLMo, BLOOM – selten!):**

- **Achtung: OSI Open Source AI Definition (opensource.org/ai):**
  - Die offizielle OSI-Definition verlangt NICHT die Trainingsdaten selbst, nur "Data Information" (Beschreibung, wie man Daten beschaffen kann)
  - D.h. Llama, Mistral etc. könnten unter OSI als "Open Source AI" gelten, obwohl sie keine Daten freigeben
  - Meine Folie ist strenger als die OSI-Definition – das ist bewusst so, da ich "echte" Offenheit zeigen will
  - Kritik: OSI-Definition verwässert den Open-Source-Gedanken

:::




## 

![Geschätzte Anzahl "Open Weights" Modelle auf huggingface.co](images/hugging-face-over-time-sketch.svg){#fig-hf}


## {background-image="images/huggingface.png" background-size="contain" background-color="white"}

:::{#huggingface}          
[](https://huggingface.co) 
:::  

## OpenWeights Cloud AI

- [mistral.ai](https://mistral.ai)
- [ollama.ai](https://ollama.ai)
- [qwen.ai](https://qwen.ai)
- [deepseek.com](https://deepseek.com)
- [Gemma](https://deepmind.google/models/gemma/)


:::{.notes}

- Mistral ist EU basiert (FR)
- qwen ist aus China (Alibaba Cloud)
- ollama cloud ist CLI basiert, und kann auch lokale AI models verwenden
- deepseek ist aus China. Eine der ersten OS AI Models, die aufgrund von Einschränkungen in der Grafikkarte mit weniger Leistung klarkommen mussten
- Gemma ist von Google


:::



# Demo: Ollama {background-image="images/lama-7024125_1280.jpg" .shadow .blurred-10px}

<!-- [OpenSource Lokal]{.shadow} -->

## {background-image="images/lama-7024125_1280.jpg"}


:::{.notes}

- Opensource Lokale KI
- OS Cloud KI sind zwar super, aber mit vorsicht zu geniessen. Sensible Daten wandern trotzdem ab und können für Trainingszwecken verwendet werden
- Lokale LLMs sind hier die Lösung

- Wir arbeiten erstmals im Terminal.
- Steiler einstieg, es gibt auch GUIS
- aber der Terminal öffnet uns Tür und Tor für FOSS

:::



## Ollama{background-color="white"}


![](images/ollama.png)



::: {.notes}

**Windows-Installation:**

1. Von [ollama.com/download](https://ollama.com/download) **OllamaSetup.exe** herunterladen
2. OllamaSetup.exe mit default Einstellungen installieren 
:::


## Terminal Öffnen

- Windows: *Command Prompt*
- Mac: *Terminal*
- Linux: *Terminal*


::: {.notes}
- Wenn der Befehl nicht gefunden wird, starten Sie den Computer neu
:::


## Im Terminal


:::{.terminal}
```{.bash}
ollama --version          
```
```{.txt}
ollama version is 0.14.2
```
:::

:::{.notes}

- Der Terminal bietet den Zugang zu OpenSource Software
- Ohne Terminal ist nur ein Bruchteil der FOSS nutzbar
- Der Terminal wirkt anfänglich befremdlich und umständlich, aber mit KI wird die Benutzung vereinfacht
- `ollama` ruft das Programm "ollama" auf
- `--version` ist ein Argument zum Program `ollama`
:::


## Im Terminal


:::{.terminal}
```{.bash}
ollama --help          
```
```{.txt}
Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  signin      Sign in to ollama.com
  signout     Sign out from ollama.com
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.

```
:::

:::{.notes}
- `ollama` ruft das Programm "ollama" auf
- `--help` ist ein Argument zum Program `ollama` und ruft die Hilfestellung zu dieser Software auf
- fast alle terminal programme haben die argumente `--version` und `--help`
:::

## RAM Verfügbarkeit{visibility="hidden"}


:::{.terminal}
```{.bash}
systeminfo 
```
```{.txt}
[...]
Gesamter physischer Speicher: 32’717 MB
[...]
```
:::

::: {.notes}
- Die meisten modernen Laptops haben mindestens 8 GB RAM
- Für den Workshop reichen kleine Modelle (1.5B - 3B) völlig aus
- Mehr RAM = größere Modelle = bessere Qualität (aber langsamer)

**Interpretation:**

- **8 GB oder weniger**: Kleine Modelle (1.5B - 3B) oder Cloud
- **16 GB**: Mittlere Modelle (7B) möglich ✅
- **32 GB+**: Große Modelle (13B+) möglich
:::

## GPU Verfügbarkeit{visibility="hidden"}

:::{.terminal}
```{.bash}
wmic path win32_VideoController get name
```

:::


::: {.notes}
- NVIDIA GPUs beschleunigen die Inferenz erheblich
- AMD/Intel GPUs werden von Ollama noch nicht gut unterstützt
- Ohne GPU dauern Antworten etwas länger, funktionieren aber trotzdem
- Für den Unterricht ist CPU-basierte Inferenz völlig ausreichend
:::

## Modellwahl

- Falls Sie mindestens 8 GB RAM haben suchen Sie auf [ollama.ai](https://ollama.ai) ein kleines Model (1 - 2b Parameter) heraus, (z.B. `llama3.2`, `deepseek-r1`)
- Falls Sie weniger als 8 GB RAM haben
  1. Erstellen Sie ein kostenloses Konto auf [ollama.com](https://ollama.com)
  2. Melden Sie sich in der Kommandozeile an

:::{.terminal}
```{.bash}
ollama signin
```
:::

:::{.notes}

- Bei gewissen Modellen (z.B: deepseek-r1) gibt es verschiedene Grössen (1 - 700b). 
- Das sind die Anzahl Parameter (in Milliarden). 
- Wir starten mit einem kleinen Modell, indem wir mit einem Klick den Namen kopieren.

:::


##

Mit folgendem Kommandozeilenbefehl kann man ein Modell (lokal oder cloud) initialisieren

:::{.terminal}
```{.bash}
# initialisiert ein kleines, lokales Modell
ollama pull deepseek-r1:1.5b     


# initialisiert ein grosses cloud Modell
ollama pull ministral   
```
:::


::: {.notes}
- `deepseek-r1:1.5b`: ~1 GB Download, sehr schnell, gut für logisches Denken
- `llama3.2:3b`: ~2 GB Download, vielseitig, populär
- `qwen2.5:3b`: ~2 GB Download, besser bei nicht-englischen Sprachen
- Der Download kann je nach Internetverbindung 1-5 Minuten dauern


```powershell
ollama pull gemini-flash      # Schnell und kosteneffizient
ollama pull deepseek-v3       # Sehr leistungsfähig
ollama pull ministral         # Vielseitig
```
:::





## Chat initialisieren


:::{.terminal}
```{.bash}
# Lokales Modell:
ollama run deepseek-r1:1.5b


# ODER Cloud-Modell:
ollama run gemini-flash
```
:::


## Erster Prompt


:::{.terminal}
```{.txt}
>>> Explain a group of environmental professionals the benifits of 
OpenSource Large Language Models
```
:::



::: {.notes}
**Wichtige Punkte:**
- `ollama run <modell>` startet eine Chat-Sitzung (cloud oder lokal)
- Die erste Abfrage kann etwas länger dauern Folgeabfragen sind schneller
- Beenden Sie den Chat mit `/bye` oder Strg+C


**Was passiert hinter den Kulissen (bei lokalen Modellen):**

1. Die Modelldatei wird in den RAM geladen
2. Ihr Text wird in Zahlen (Tokens) umgewandelt
3. Das Modell (die Gewichte) berechnet das nächste wahrscheinlichste Token
4. Die Ergebnisse werden wieder in Text umgewandelt
5. Alles geschieht **lokal auf Ihrem Computer** - komplett offline!


Jetzt haben wir einen etwas langsamen und nicht sehr intelligenten Chatbox.. was hat uns das gebracht?
:::





## Die drei Verbesserungen{visibility="hidden"}

::: {.columns}
::: {.column width="33%"}
**RAG**

Retrieval Augmented Generation

Geben Sie LLM Zugriff auf IHRE Dokumente

*Beispiel: IUNR-Kursmaterialien, Forschungsarbeiten*
:::

::: {.column width="33%"}
**MCPs**

Modellkontextprotokoll

Verbinden Sie LLM mit Ihren Tools

*Beispiel: Felddaten, GIS, E-Mail, Kalender*
:::

::: {.column width="33%"}
**Personas**

Systemaufforderungen

Gestalten Sie das Verhalten von LLM

*Beispiel: "Lehrassistent für Umweltwissenschaften"*
:::
:::

::: {.notes}
Diese drei Verbesserungen machen lokale LLMs leistungsstark. Gehen Sie nicht zu sehr ins Technische – erklären Sie einfach, was jede einzelne Funktion bewirkt, und geben Sie konkrete Beispiele, die für den Unterricht relevant sind. RAG ist für ihren Anwendungsfall besonders wichtig.
:::


# Demo AnythingLLM {background-image="images/lama-91978_1280.jpg" .shadow .blurred-5px}

<!-- [LLMs via einem GUI]{.shadow} -->

:::{.notes}

- Anything LLM

:::

## {background-image="images/lama-91978_1280.jpg"}


## AnythingLLM

- eine ideale Ergänzung zu Ollama
- bietet ein grafisches Interface und verwendet die Modelle, die von Ollama zu Verfügung gestellt werden
- ist frei und OpenSource, und für alle 3 Plattformen verfügbar



## Ollama mit AnythingLLM


:::{.r-stack}
![](images/anything-llm-0.png)

![](images/anything-llm-2.png){.fragment}

![](images/anything-llm-3.png){.fragment}

![](images/anything-llm-5.png){.fragment}

:::

:::{.notes}

- Dateien Hinzufügen
- Konvertierung zu RAG (lokal)
  - Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources.[1] With RAG, LLMs do not respond to user queries until they refer to a specified set of documents.
- Kontext basiertes chatten
:::



# Demo R mit Ollama {background-image="images/losangeles-7754986_1280.jpg" .shadow .blurred-5px}


## {background-image="images/losangeles-7754986_1280.jpg"}


## Züri Wie Neu

±70'000 Meldungen aus einer online Plattform zur Meldung von Schäden an der städtischen Infrastruktur. Jede Einträg enthält Koordinaten, Text und Kategorie.

- [Aufgabe:]{.underline}: Meldungen kategorisieren in verschiedene Dringlichkeitsstufen: Tief, Mittel, Hoch
- [Ansatz:]{.underline}:
  - Jeder Task in einem neuen chat (Kontext Limitierung)
  - Few shot: Ein paar wenige Beispiele zur Orientierung


##

```{.txt filename="prompt.txt"}
Klassifiziere diese Meldung nach Dringlichkeit.

Beispiele:
- 'Grosses Loch in der Strasse, gefährlich für Velos und Töffs!' -> HOCH
- 'Schachtdeckel im Trottoir Stolpergefahr' -> HOCH
- 'Schlagloch in der Strasse beim Fussgängerstreifen' -> MITTEL
- 'Lampe defekt' -> MITTEL
- 'Strassenbennungstafel ist krumm' -> TIEF
- 'Pfosten ist verbogen' -> TIEF

HOCH = Sicherheitsrisiko, Verletzungsgefahr
MITTEL = Funktionsproblem, störend
TIEF = Kosmetisch, nicht dringend

Antworte NUR mit: HOCH, MITTEL oder TIEF

Die Meldung lautet wie folgt:
```

## Züri Wie Neu


```{.r  filename="analysis.R" code-line-numbers="|2|4-5|7|9,14|10|12-13"}
library(readr); library(dplyr); library(purrr); library(stringr)
library(ollamar)

meldungen <- read_csv("zueri-wie-neu.csv") |>
  slice_sample(n = 20)

prompt <- readLines("prompt.txt") |> 
  paste(collapse = " ")

meldungen$severity <- map_chr(meldungen$detail, \(x) {
  prompt_i <- paste0(prompt, x)
  
  generate("llama2:13b", prompt_i) |>
    resp_process("text")
}, .progress = TRUE)
```

:::{.notes}
- Das verwendete Modell muss lokal vorhanden sein (Terminal: `ollama list`)
- Antworten müssen im Anschluss noch bereinigt werden
- Quantitativ testen (Testdatensatz erstellen), mit verschiedenen Modellen!
- Keine Kontext Limitierung, keine Limitierung der Laufzeit (eigene Hardware)
- Erweiterung:

:::


## Erweiterung

| Erweiterung      | Nutzen                                                               |
|------------------|----------------------------------------------------------------------|
| JSON Output      | Severity + Begründung                                                |
| Entscheidungslog | Entscheidungen tracken, als Few-shot Beispiele für Konsistenz nutzen |
| Konfidenz-Score  | "SICHERHEIT: 1-5" hinzufügen um unsichere Fälle zu erkennen          |
| Batch-Kontext    | Letzte 3 Klassifikationen zeigen um die Skala zu verankern           |


# GrüentAI {background-image="images/zhaw-gruental-905126769.jpg" .shadow .blurred-5px}

::: {.attribution}
Photo courtesy of [ZHAW](https://www.zhaw.ch/de/lsfm/ueber-uns)
:::

## {background-image="images/zhaw-gruental-905126769.jpg"}

::: {.attribution}
Photo courtesy of [ZHAW](https://www.zhaw.ch/de/lsfm/ueber-uns)
:::

##


:::{.blockquote}
Koennen lokal betriebene LLM-Modelle Bachelorarbeiten nach dem ZHAW-Schema bewerten und dabei eine Abweichung von weniger als 0.5 Notenpunkten zur manuellen Bewertung erzielen?
:::

## Bewertungsschema

| Kategorie            | Gewicht | Bewertungsfokus                         |
|----------------------|---------|-----------------------------------------|
| Inhalt               | 40%     | Stand des Wissens, Methodik, Relevanz   |
| Aufbau               | 10%     | Roter Faden, logische Struktur          |
| Darstellung          | 10%     | Formatierung, Abbildungen, Tabellen     |
| Sprache              | 10%     | Wissenschaftlicher Stil, Grammatik      |
| Literaturangabe      | 10%     | Zitierweise, Quellenarbeit              |
| Allgemeiner Eindruck | 20%     | Wissenschaftlichkeit, Eigenstaendigkeit |


## {background-image="images/abweichung_pro_modell.svg" background-size="contain"}

## {background-image="images/manual_vs_ai_grade.svg" background-size="contain"}

:::{.notes}

- Fazit: Die vorgeschlagenen Noten sind nicht wirklich brauchbar 
- Allerdings waren die Begründungen, die ebenfalls als Output generiert wurden hilfreich für die manuelle Benotung

:::


# Take Home Messages {background-image="images/notebook-1840276_1280.jpg" .shadow .blurred-5px}

## {background-image="images/notebook-1840276_1280.jpg"}

## 

:::{.incremental}
1. Vendor Lock-In vermeiden: Kontrolle über eigene Daten behalten
1. KI verändert unsere Arbeit: Besser aktiv mitgestalten als reagieren
1. Lokale KI ist heute nutzbar: Ollama + AnythingLLM auf jedem Laptop
1. Open Weights ≠ Open Source, aber für die Praxis oft ausreichend
1. Der Terminal ist kein Hindernis, sondern das Tor zu FOSS
:::

## Was noch fehlt 

- In Bezug auf KI gibt es noch viele ungelöste Probleme und reale Risiken.
- Empfehlung: Robert Miles ([@RobertMilesAI](https://www.youtube.com/@RobertMilesAI)) — verständliche Erklärungen zu AI Safety

## {background-image="images/pexels-gratisography-519.jpg"}





